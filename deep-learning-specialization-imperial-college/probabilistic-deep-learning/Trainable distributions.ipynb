{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cubic-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "refined-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = tfd.Normal(loc=tf.Variable(0., name='loc'), scale=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "registered-swaziland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'loc:0' shape=() dtype=float32, numpy=0.0>,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-dialogue",
   "metadata": {},
   "source": [
    "The mean value of this normal distribution is now trainable and can be updated accordingly to some learning principle. The learning principle that we often use when training DL models is maximum likelihood. This is the goal of finding the parameters of our model that maximize the likelihood or probability of our data. Finding the parameters that maximize the likelihood is the same as finding the parameters that minimize the negative log likelihood and in practice this is what is often used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "respective-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(x_train):\n",
    "    return -tf.reduce_mean(normal.log_prob(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-island",
   "metadata": {},
   "source": [
    "Let's define a custom training loop to learn the mean parameters from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "missing-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_loss_and_grads(x_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(normal.trainable_variables)\n",
    "        loss=nll(x_train)\n",
    "    grads = tape.gradient(loss, normal.trainable_variables)\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adjustable-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples = tfd.Normal(10, 1).sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fleet-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
    "\n",
    "num_steps = 10\n",
    "for _ in range(num_steps):\n",
    "    loss, grads = get_loss_and_grads(x_samples)\n",
    "    optimizer.apply_gradients(zip(grads, normal.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-director",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
