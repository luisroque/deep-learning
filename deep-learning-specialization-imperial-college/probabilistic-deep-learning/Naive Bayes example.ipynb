{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "flexible-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "utility-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "detected-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a function to get data which:\n",
    "#   1) Fetches the 20 newsgroups dataset\n",
    "#   2) Performs a word count on the articles and binarizes the result\n",
    "#   3) Returns the data as a numpy matrix with the labels\n",
    "\n",
    "def get_data(categories):\n",
    "    \n",
    "    newsgroups_train_data = fetch_20newsgroups(data_home='20_Newsgroup_Data/',\n",
    "                                              subset='train', categories=categories)\n",
    "    newsgroups_test_data = fetch_20newsgroups(data_home='20_Newsgroup_Data/',\n",
    "                                             subset='test', categories=categories)\n",
    "    \n",
    "    n_documents = len(newsgroups_train_data['data'])\n",
    "    \n",
    "    # Count words occurences and binarize the result filtering out\n",
    "    # excessively common words or words that only appear once in our corpus\n",
    "    count_vectorizer = CountVectorizer(input='content', \n",
    "                                       binary=True, \n",
    "                                       max_df=0.25, \n",
    "                                       min_df=1.01/n_documents)\n",
    "    \n",
    "    train_binary_bag_of_words = count_vectorizer.fit_transform(newsgroups_train_data['data'])\n",
    "    test_binary_bag_of_words = count_vectorizer.transform(newsgroups_test_data['data'])\n",
    "    \n",
    "    return (train_binary_bag_of_words.todense(), newsgroups_train_data['target']), (test_binary_bag_of_words.todense(), newsgroups_test_data['target']), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "specific-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to conduct Laplace smoothing. This adds a base level of probability for a given feature\n",
    "# to occur in every class\n",
    "\n",
    "def laplace_smoothing(labels, binary_data, n_classes):\n",
    "    # Compute the parameter estimates (adjusted fraction of documents in class that contain word)\n",
    "    n_words = binary_data.shape[1]\n",
    "    alpha = 1 # parameter for the Laplace smoothing\n",
    "    theta = np.zeros([n_classes, n_words]) # stores parameter values - prob. word given class\n",
    "    for c_k in range(n_classes): # 0, 1, ..., 19\n",
    "        class_mask = (labels == c_k)\n",
    "        N = class_mask.sum() # number of articles in class\n",
    "        theta[c_k,:] = (binary_data[class_mask, :].sum(axis=0) + alpha)/(N + alpha*2)\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "chronic-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a subset of the 20 newsgroups dataset\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = get_data(categories)\n",
    "smoothed_counts = laplace_smoothing(labels=train_labels, binary_data=train_data, n_classes=len(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-flower",
   "metadata": {},
   "source": [
    "To make our NB classifier we need to build three functions:\n",
    "\n",
    "* Compute the class priors\n",
    "* Build our class conditional statements\n",
    "* Put it all together and classify our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "distinguished-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which computes the prior probability of every class based on the frequency of occurence in\n",
    "# the dataset\n",
    "\n",
    "def class_priors(n_classes, labels):\n",
    "    counts = np.zeros(n_classes)\n",
    "    for c_k in range(n_classes):\n",
    "        counts[c_k] = np.sum(np.where(labels==c_k, 1, 0))\n",
    "    priors = counts/np.sum(counts)\n",
    "    print('The class priors are {}'.format(priors))\n",
    "    return priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "stylish-grenada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class priors are [0.2359882  0.28711898 0.29154376 0.18534907]\n"
     ]
    }
   ],
   "source": [
    "# Run the function\n",
    "\n",
    "priors = class_priors(n_classes=len(categories), labels=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "functioning-affect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.Independent 'IndependentBernoulli' batch_shape=[4] event_shape=[17495] dtype=int32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will do a function that given the feature occurence counts returns a Bernoulli distribution of\n",
    "# batch_shape = number of classes and event_shape = number of features\n",
    "\n",
    "def make_distribution(probs):\n",
    "    batch_of_bernoullis = tfd.Bernoulli(probs=probs)\n",
    "    dist = tfd.Independent(batch_of_bernoullis,\n",
    "                          reinterpreted_batch_ndims=1)\n",
    "    return dist\n",
    "\n",
    "tf_dist = make_distribution(smoothed_counts)\n",
    "tf_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "continuous-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final function predict_sample which given the distribution, a test sample, and the class priors:\n",
    "#    1) Computes the class conditional probabilities given the sample\n",
    "#    2) Forms the joint likelihood\n",
    "#    3) Normalizes the joint likelihood and returns the log prob\n",
    "\n",
    "def predict_sample(dist, sample, priors):\n",
    "    cond_probs = dist.log_prob(sample)\n",
    "    joint_likelihood = tf.add(np.log(priors), cond_probs)\n",
    "    norm_factor = tf.math.reduce_logsumexp(joint_likelihood, axis=-1, keepdims=True)\n",
    "    log_prob = joint_likelihood - norm_factor\n",
    "    \n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-circus",
   "metadata": {},
   "source": [
    "#### Computing log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "portable-great",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=\n",
       "array([-6.1736359e+01, -1.5258789e-05, -1.1619873e+01, -6.3327652e+01],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting one example from our test data\n",
    "\n",
    "log_probs = predict_sample(tf_dist, test_data[0], priors)\n",
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "express-maple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1  0.7848499112849504\n"
     ]
    }
   ],
   "source": [
    "# Loop over our test data and classify\n",
    "\n",
    "probabilities =[]\n",
    "for sample, label in zip(test_data, test_labels):\n",
    "    probabilities.append(tf.exp(predict_sample(tf_dist, sample, priors)))\n",
    "    \n",
    "probabilities = np.asarray(probabilities)\n",
    "predicted_classes = np.argmax(probabilities, axis=-1)\n",
    "print('f1 ', f1_score(test_labels, predicted_classes, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "italian-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 from sklearn 0.7848499112849504\n"
     ]
    }
   ],
   "source": [
    "# Make a Bernoulli Naive Bayes classifier using sklearn with the same leevl of alpha smoothing\n",
    "\n",
    "clf = BernoulliNB(alpha=1)\n",
    "clf.fit(train_data, train_labels)\n",
    "pred = clf.predict(test_data)\n",
    "print('f1 from sklearn', f1_score(test_labels, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-filter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
