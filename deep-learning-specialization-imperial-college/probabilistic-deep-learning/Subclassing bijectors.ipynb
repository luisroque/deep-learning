{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "marked-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "confirmed-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySigmoid(tfb.Bijector):\n",
    "    \n",
    "    def __init__(self, validate_args=False, name='sigmoid'):\n",
    "        super(MySigmoid, self).__init__(\n",
    "            validate_args=validate_args, forward_min_event_ndims=0, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-leone",
   "metadata": {},
   "source": [
    "We are passing the argument `forward_min_event_ndims` to the class initializer. When you are creating a bijector you need to set the minimum number of event dimensions that the bijector needs to act upon. In this case, the sigmoid is a function that can operate on scalars, so the minimum number of dimensions is set to zero.\n",
    "\n",
    "For instance, in the softmax centered bijector the inputs need to have at least rank 1. \n",
    "\n",
    "You need to supply the `forward_min_event_ndims` argument of the `inverse_min_event_ndims` argument. In most cases you just need to supply one of them to the base class constructor. However, some bijectors might change the shape of the input, so it is possible that the minimum number of event dimensions is different in the forward and in the inverse transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intelligent-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySigmoid(tfb.Bijector):\n",
    "    \n",
    "    def __init__(self, validate_args=False, name='sigmoid'):\n",
    "        super(MySigmoid, self).__init__(\n",
    "            validate_args=validate_args, forward_min_event_ndims=0, name=name)\n",
    "        \n",
    "    def _forward(self, x):\n",
    "        return tf.math.sigmoid(x)\n",
    "    \n",
    "    def _inverse(self, y):\n",
    "        return tf.math.log(y) - tf.math.log(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-trance",
   "metadata": {},
   "source": [
    "Next, we need to define the forward and inverse tranformations of the bijector. These should be implemented as the `_forward` and `_inverse` methods of the class. In this example, we are using the built in sigmoid function. The inverse of the sigmoid is the logit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "crazy-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySigmoid(tfb.Bijector):\n",
    "    \n",
    "    def __init__(self, validate_args=False, name='sigmoid'):\n",
    "        super(MySigmoid, self).__init__(\n",
    "            validate_args=validate_args, forward_min_event_ndims=0, name=name)\n",
    "        \n",
    "    def _forward(self, x):\n",
    "        return tf.math.sigmoid(x)\n",
    "    \n",
    "    def _inverse(self, y):\n",
    "        return tf.math.log(y) - tf.math.log(1-y)\n",
    "    \n",
    "    def _inverse_log_det_jacobian(self, y):\n",
    "        return -tf.math.log(y) - tf.math.log(1-y)\n",
    "    \n",
    "    def _forward_log_det_jacobian(self, y):\n",
    "        return -tf.math.softplus(-x) - tf.math.log(x)\n",
    "        #return -self._inverse_log_det_jacobian(self._forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-pregnancy",
   "metadata": {},
   "source": [
    "The log jacobian computation should return a scalar for each event.\n",
    "\n",
    "Besides we know that the inverse of the log jacobian determinant is just the inverse of the forward log jacobian, so we could, for instance, implement the forward log det jacobian using this fact. So, in fact, we only need to supply one of the log det jacobian methods and the inverse will be computed automatically in the form written in a comment above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "buried-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyShift(tfb.Bijector):\n",
    "    \n",
    "    def __init__(self, shift, validate_args=False, name='shift'):\n",
    "        self.shift = shift\n",
    "        super(MyShift, self).__init__(\n",
    "        validate_args=validate_args, forward_min_event_ndims=0, name=name,\n",
    "        is_constant_jacobian=True)\n",
    "        \n",
    "    def _forward(self, x):\n",
    "        return x + self.shift\n",
    "    \n",
    "    def _inverse(self, y):\n",
    "        return y - self, shift\n",
    "    \n",
    "    def _forward_log_det_jacobian(self, x):\n",
    "        return tf.constant(0, x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-gazette",
   "metadata": {},
   "source": [
    "The keyword argument `is_constant_jacobian` can be set to true when the log jacobian determinant of the bijector is independent of the input. An example is showned above for a simple shift bijector. In this case the jacobian is a constant zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-sport",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
