{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bulgarian-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, Conv2DTranspose\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "external-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z ~ N(0, I) = p(z)\n",
    "# p(x|z) = decoder\n",
    "# x ~ p(x|z)\n",
    "\n",
    "# encoder(x) = q(z|x) \\approx p(z|x)\n",
    "# log p(x) >= E_{z~q(z|x)}[-log q(z|x) + log p(x, z)]\n",
    "#           = -KL(q(z|x) || p(z)) + E_{z~q(z|x)}[log p(x|z)] (ELBO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-participation",
   "metadata": {},
   "source": [
    "The first two lines represent the generative model that underlies the derivation of the algorithm. The modeling assumptions is that the data examples are generated according to the following process: we first sample a latent variable z (distributed by a simple distribution, such as a zero mean isotropic Gaussian). This is the prior distribution. The sample z is then transformed by some function, which here we are denoting as decoder which paramterizes a distribution from which we can sample a data point x. The algorithm requires us to also define an encoder or recognition network, that you often see denoted as q. This encoder network takes a data example x as input and outputs a distribution over the latent variable z. This distribution is an approximation of the true posterior distribution of our model p(z|x). The objective that we want to maximize is the ELBO. This objective can be written as the sum of two terms: the first is the negative KL divergence between the prior p(z) and the posterior q(z|x). The second term is the expected log likelihood under the approximate posterior distribution of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "weekly-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size=2\n",
    "event_shape=(28,28,1)\n",
    "\n",
    "encoder = Sequential([\n",
    "    Conv2D(8, (5, 5), strides=2, activation='tanh', input_shape=event_shape),\n",
    "    Conv2D(8, (5, 5), strides=2, activation='tanh'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='tanh'),\n",
    "    Dense(2*latent_size),\n",
    "    tfpl.DistributionLambda(lambda t: tfd.MultivariateNormalDiag(\n",
    "        loc=t[..., :latent_size], scale_diag=tf.math.exp(t[..., latent_size:])))\n",
    "], name='encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-vault",
   "metadata": {},
   "source": [
    "The last dense layer has 4 units, which is what is required to parameterize a two-dimensional Gaussian distribution. The final layer is a `DistributionLambda` layer which returns a multivariate normal diag distribution of the approximate posterior. You can see that for the keyword arguments loc and scale_diag are receiving slices of the input tensor from the previous dense layer. Overall you can see how this encoder network is progressively compressing the input through the layers until it has encoded into a two-dimensional latent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-photography",
   "metadata": {},
   "source": [
    "If we pass in a batch of data examples of batch size 16, then the encoder network returns a multivariate normal diag distribution object with batch shape 16 and an event shape of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "japanese-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Sequential([\n",
    "    Dense(64, activation='tanh', input_shape=(latent_size,)),\n",
    "    Dense(128, activation='tanh'),\n",
    "    Reshape((4, 4, 8)),\n",
    "    Conv2DTranspose(8, (5, 5), strides=2, output_padding=1, activation='tanh'),\n",
    "    Conv2DTranspose(8, (5, 5), strides=2, output_padding=1, activation='tanh'),\n",
    "    Conv2D(1, (3, 3), padding='SAME'),\n",
    "    Flatten(),\n",
    "    tfpl.IndependentBernoulli(event_shape)\n",
    "], name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-recruitment",
   "metadata": {},
   "source": [
    "The decoder network is progressively expanding its inputs through the layers. The input to the decoder is a length 2 vector, which will be sample of the latent variable. The layers are roughly the inverse of what we have in the encoder. Up to the last Conv2D (inclusive) we have up sample the input back to the spatial dimensions of the data. We then use a Conv2D layer to reduce the filters down to 1. It does not uses any activation function so the activations can have any real value. Finally, we use an Independent Bernoulli distribution with the same event_space of the data. This distribution will take the input tensor from the flatten layer as the logits to parameterize the Bernoulli distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "breeding-industry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.Independent 'decoder_independent_bernoulli_IndependentBernoulli_Independentdecoder_independent_bernoulli_IndependentBernoulli_Bernoulli' batch_shape=[16] event_shape=[28, 28, 1] dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(tf.random.normal([16, latent_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-entry",
   "metadata": {},
   "source": [
    "The decoder outputs a batched independent Bernoulli distribution with batch shape equal to 16 and event shape equal to (28, 28, 1), which is the data event_shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "married-comfort",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.MultivariateNormalDiag 'MultivariateNormalDiag' batch_shape=[] event_shape=[2] dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior = tfd.MultivariateNormalDiag(loc=tf.zeros(latent_size))\n",
    "prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "limited-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x_true, approx_posterior, x_pred, prior_dist):\n",
    "    return tf.reduce_mean(tfd.kl_divergence(approx_posterior, prior_dist)\n",
    "                         - x_pred.log_prob(x_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-vancouver",
   "metadata": {},
   "source": [
    "Our loss function is the negative of the ELBO. We want to maximize the ELBO, meaning that we want to minimize our loss function.\n",
    "\n",
    "The loss function reveives a batch of data examples `x_true`. The `approx` posterior is the output of the encoder, which is a multivariate normal diag distribution. `x_pred` is the output of the decoder which is an independent Bernoulli distribution. Finally, we are also passing in the prior distribution.\n",
    "\n",
    "When computing the loss we are in fact computing the KL divergence between the posterior and the prior and we are subtracting the likelihood term which is the negative log probability of the data example x_true according to the decoder prediction x_pred. It returns the mean over the batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "operational-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x_true, approx_posterior, x_pred, prior_dist):\n",
    "    reconstruction_loss = -x_pred.log_prob(x_true)\n",
    "    approx_posterior_sample = approx_posterior.sample()\n",
    "    kl_approx = (approx_posterior.log_prob(approx_posterior_sample)\n",
    "                - prior_dist.log_prob(approx_posterior_sample))\n",
    "    return tf.reduce_mean(kl_approx + reconstruction_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-error",
   "metadata": {},
   "source": [
    "We can also compute the KL divergence using Monte Carlo samples, instead of doing it analytically using the `kl_divergence` function. Depending on the choice of prior and posterior the MC sample can be required to compute the KL divergence. The `reconstruction_loss` is the likelihood term. Then we sample from the posterior (in this case we are only sampling once). The KL divergence is computed by calculating the log probability of the posterior using the sample from the posterior and subtracting the result by the prior distribution log probability also using the sample from the posterior. The difference between these two log probabilities gives the Monte Carlo estimate. This estimate is then added to the `reconstruction_loss` and returned as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "indie-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_loss_and_grads(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        approx_posterior = encoder(x)\n",
    "        approx_posterior_sample = approx_posterior.sample()\n",
    "        x_pred = decoder(approx_posterior_sample)\n",
    "        current_loss = loss_fn(x, approx_posterior, x_pred, prior)\n",
    "    grads = tape.gradient(current_loss, encoder.training_variables + decoder.training_variables)\n",
    "    return current_loss, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-paper",
   "metadata": {},
   "source": [
    "This function takes a batch of data examples x as input and executes the network computations inside the GradienTape context. First, we compute the approx_posterior distribution by passing x through the encoder. We then compute the output distribution by taking a sample from the approximate posterior and feeding it to the decoder. So `x_pred` is a Bernoulli distribution object that is trying to reconstruct the data input x. The gradients are computed by calling the `tape.gradient` and the variables that we want to get gradients for are the collection of trainable variables from encoder and decoder networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam()\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for train_batch in train_data:\n",
    "        loss, grads = get_loss_and_grads(train_batch)\n",
    "        opt.apply_gradients(zip(grads, encoder.trainable_variables + decoder.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae(inputs):\n",
    "    approx_posterior = encoder(inputs)\n",
    "    decoded = decoder(approx_posterior.sample())\n",
    "    return decoded.sample()\n",
    "\n",
    "reconstruction = vae(x_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-plasma",
   "metadata": {},
   "source": [
    "We can define the end-to-end bottleneck architecture. The function above takes a batch of data examples as input and passes them through the encoder, samples from the approx posterior and passes the samples through the decoder, before sampling again from the output distribution returned by the decoder. This sample should be an approximate reconstruction of the original data input. So passing it `x_sample` it will return a tensor with the same shape as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-seattle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
