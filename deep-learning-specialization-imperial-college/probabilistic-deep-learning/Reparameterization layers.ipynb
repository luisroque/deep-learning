{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "occasional-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, MaxPool2D\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "light-slide",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    tfpl.Convolution2DReparameterization(16, \n",
    "                                         [3, 3], \n",
    "                                         activation='relu', \n",
    "                                         input_shape=(28, 28, 1),\n",
    "                                         kernel_posterior_fn=tfpl.default_mean_field_normal_fn(),\n",
    "                                         kernel_prior_fn=tfpl.default_multivariate_normal_fn),\n",
    "    MaxPool2D(3),\n",
    "    Flatten(),\n",
    "    tfpl.DenseReparameterization(tfpl.OneHotCategorical.params_size(10)),\n",
    "    tfpl.OneHotCategorical(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-bouquet",
   "metadata": {},
   "source": [
    "The callable function `default_mean_field_normal_fn` returns an independent normal distribution with trainable mean and standard deviation parameters. So the default posterior distribution for the reparameterization layers is similar to example we saw about the dense variational layers. The prior distribution is defined by the `default_multivariate_normal_fn` function, which is again a function defined in the tfp layers module. This function returns a spherical Gaussian with zero mean and standard deviation equal to 1 for each component (the same for the dense variational example).\n",
    "\n",
    "We could change these defaults and there is a required specification for any callable that we pass to either the kernel posterior or kernel prior function keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "reliable-thinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_multivariate_normal(dtype, shape, name, trainable, add_variable_fn):\n",
    "    normal = tfd.Normal(loc=tf.zeros(shape, dtype), scale=2*tf.ones(shape, dtype))\n",
    "    batch_ndims = tf.size(normal.batch_shape_tensor())\n",
    "    return tfd.Independent(normal, reinterpreted_batch_ndims=batch_ndims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-annotation",
   "metadata": {},
   "source": [
    "The callable should, as before, return a distribution object. This is a simple example of a function that could be used to change the standard deviation of the prior distribution. The default multivariate normal function returns a spherical gaussian with standard deviation of 1. Here we changed the prior to have a larger standard deviation of 2. We are also using the Independent distribution to make sure that the dimensions are part of the event space of the distribution.\n",
    "\n",
    "Now, we can use it on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    tfpl.Convolution2DReparameterization(16, \n",
    "                                         [3, 3], \n",
    "                                         activation='relu', \n",
    "                                         input_shape=(28, 28, 1),\n",
    "                                         kernel_posterior_fn=tfpl.default_mean_field_normal_fn(),\n",
    "                                         kernel_prior_fn=tfpl.custom_multivariate_normal_fn),\n",
    "    MaxPool2D(3),\n",
    "    Flatten(),\n",
    "    tfpl.DenseReparameterization(tfpl.OneHotCategorical.params_size(10)),\n",
    "    tfpl.OneHotCategorical(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-certification",
   "metadata": {},
   "source": [
    "The bias parameters also have prior and posterior function keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    tfpl.Convolution2DReparameterization(16, \n",
    "                                         [3, 3], \n",
    "                                         activation='relu', \n",
    "                                         input_shape=(28, 28, 1),\n",
    "                                         kernel_posterior_fn=tfpl.default_mean_field_normal_fn(),\n",
    "                                         kernel_prior_fn=tfpl.default_multivariate_normal_fn\n",
    "                                         bias_posterior_fn = tfpl.default_mean_field_normal_fn(is_singular=True),\n",
    "                                         bias_prior_fn=None),\n",
    "    MaxPool2D(3),\n",
    "    Flatten(),\n",
    "    tfpl.DenseReparameterization(tfpl.OneHotCategorical.params_size(10)),\n",
    "    tfpl.OneHotCategorical(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-lindsay",
   "metadata": {},
   "source": [
    "On the `bias_posterior_fn` we defined the `default_mean_field_normal_fn` but with the `is_singular` keyword as true. This way the bias point becomes a point estimate, instead of being represented by a distribution. This means that the callable that is returned is a deterministic distribution object, which is not a distribution in reality, it just returns a value given by its loc parameter and it is defined to have probability 1 at that value and 0 elsewhere.\n",
    "\n",
    "So the bias parameter is being learned in the same way as regular convolutional layers but it is fixed to a single value. This means that we are not using a prior distribution for the bias and so the `bias_prior_fn` is set to none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "individual-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    tfpl.Convolution2DReparameterization(16, \n",
    "                                         [3, 3], \n",
    "                                         activation='relu', \n",
    "                                         input_shape=(28, 28, 1),\n",
    "                                         kernel_posterior_tensor_fn=tfd.Distribution.sample,\n",
    "                                         kernel_posterior_fn=tfpl.default_mean_field_normal_fn(),\n",
    "                                         kernel_prior_fn=tfpl.default_multivariate_normal_fn,\n",
    "                                         bias_posterior_tensor_fn=tfd.Distribution.sample,\n",
    "                                         bias_posterior_fn = tfpl.default_mean_field_normal_fn(is_singular=True),\n",
    "                                         bias_prior_fn=None),\n",
    "    MaxPool2D(3),\n",
    "    Flatten(),\n",
    "    tfpl.DenseReparameterization(tfpl.OneHotCategorical.params_size(10)),\n",
    "    tfpl.OneHotCategorical(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-aspect",
   "metadata": {},
   "source": [
    "Here we also define the keyword argument `kernel_posterior_tensor_fn`. This function is used to compute the forward pass of the network as well as approximating the KL divergence if the option `kl_use_exact` is set to false. We also need to specify  how to convert the posterior distribution to a tensor. In a reparameterization layer this option is passed to `kernel_posterior_tensor_fn` and `bias_posterior_tensor_fn` arguments. The default is again just sampling from the distribution. In the case of the bias, since it is now just a deterministic quantity, sampling from the deterministic distribution simply returns the value of the loc parameter. It wouldn't make a difference if we pass the mean method instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    tfpl.Convolution2DReparameterization(16, \n",
    "                                         [3, 3], \n",
    "                                         activation='relu', \n",
    "                                         input_shape=(28, 28, 1),\n",
    "                                         kernel_posterior_tensor_fn=tfd.Distribution.sample,\n",
    "                                         kernel_posterior_fn=tfpl.default_mean_field_normal_fn(),\n",
    "                                         kernel_prior_fn=tfpl.default_multivariate_normal_fn,\n",
    "                                         bias_posterior_tensor_fn=tfd.Distribution.sample,\n",
    "                                         bias_posterior_fn = tfpl.default_mean_field_normal_fn(is_singular=True),\n",
    "                                         bias_prior_fn=None,\n",
    "                                         kernel_divergence_fn=(lambda q, p, _: tfd.kl_divergence(q,p) / dataset_size)),\n",
    "    MaxPool2D(3),\n",
    "    Flatten(),\n",
    "    tfpl.DenseReparameterization(tfpl.OneHotCategorical.params_size(10)),\n",
    "    tfpl.OneHotCategorical(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-springfield",
   "metadata": {},
   "source": [
    "Finally, we can also set a function to compute the KL divergence between posterior and prior distributions. The default is a lambda function that takes the posterior, prior and an unused argument and returns the result of the KL divergence on q and p. This function comes from the distributions module. This default attempts to compute the KL divergence analytically. This may or may not be possible depending on the choice of posterior and prior distributions. So if it isn't possible we will get an error. The unused parameter of the lambda function is the tensor obtained from applying the posterior tensor function to the posterior distribution. We could use this argument to approximate the KL divergence instead of computing it analytically. The same issue with the scaling of the KL divergence loss also happens on these reparameterization layers. In order to make sure that the value of our objective is equal to the negative of the ELBO we need to scale the KL divergence terms by a factor of 1/dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-insertion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
