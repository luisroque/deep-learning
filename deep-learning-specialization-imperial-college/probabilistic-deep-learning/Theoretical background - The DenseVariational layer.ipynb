{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "frank-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "western-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tfd.Normal(loc=np.arange(8, dtype=np.float32),scale=1).sample(100)\n",
    "e = tfd.Normal(loc=np.zeros(8, dtype=np.float32),scale=1).sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "coordinate-rachel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "attended-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = x_train + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "greater-crime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_train[:, 0], y_train[:, 0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-registration",
   "metadata": {},
   "source": [
    "### Modeling aleatoric uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-intermediate",
   "metadata": {},
   "source": [
    "Let's start by implementing a simple feedforward network with a probabilistic layer output. This probabilistic layer outputs is what allows us to model the aleatoric uncertainty (uncertainty or noise in the data). The network outputs a normal distribution object with a one dimensional event space where the mean and variance parameters are learned by the network. The network is trained by minimizing the negative log likelihood loss. The mean and variance of the output distribution are defined by the weights and biases present in the previous layers (before the output layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "raised-turning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22048490.4000\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8886116.0000\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 4445791.4000\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 904us/step - loss: 3421699.6500\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2393041.0000\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1741113.4500\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1405012.7750\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1095874.1000\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 996us/step - loss: 1072872.7625\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 939us/step - loss: 904769.7000\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 715729.6875\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 518378.6625\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 387586.0062\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 337193.2188\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 305327.9125\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 930us/step - loss: 217474.2188\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 180994.7125\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 910us/step - loss: 157250.9094\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 945us/step - loss: 123560.3156\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 102999.5422\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 885us/step - loss: 88951.3938\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 887us/step - loss: 75071.4688\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 62342.8664\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 972us/step - loss: 48364.4477\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 949us/step - loss: 41580.5742\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 33462.8301\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 965us/step - loss: 28566.6074\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 940us/step - loss: 24869.5551\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19089.2551\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 906us/step - loss: 16388.9629\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 900us/step - loss: 13788.2828\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 10439.6732\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 863us/step - loss: 9692.2662\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 977us/step - loss: 7415.8802\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 6267.8498\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 907us/step - loss: 5061.8940\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 4174.4097\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3177.5455\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2785.1459\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2253.9897\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1903.8449\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 989us/step - loss: 1545.3213\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1202.9478\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1012.1325\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 849.9902\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 775.0730\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s 878us/step - loss: 612.7698\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s 959us/step - loss: 526.8276\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 463.5364\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s 870us/step - loss: 370.7355\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s 847us/step - loss: 324.7044\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 280.5723\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s 933us/step - loss: 246.2868\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s 890us/step - loss: 202.3007\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 172.0463\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 157.4857\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s 897us/step - loss: 130.1063\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 114.8451\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s 880us/step - loss: 96.1823\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 83.9858\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 76.0862\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 67.8458\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 913us/step - loss: 59.4253\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 54.9146\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 915us/step - loss: 49.0922\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 870us/step - loss: 44.5714\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 40.5402\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 965us/step - loss: 37.6264\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 853us/step - loss: 34.3395\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 31.7701\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 30.1135\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 954us/step - loss: 28.5087\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 27.0039\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 928us/step - loss: 25.6401\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 847us/step - loss: 24.5611\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 23.7099\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 22.8223\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 875us/step - loss: 22.1715\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 21.6930\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 887us/step - loss: 21.1022\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 890us/step - loss: 20.7074\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 20.4127\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 943us/step - loss: 20.0949\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s 831us/step - loss: 19.8946\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.7232\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 999us/step - loss: 19.5296\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.4293\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.3732\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.2391\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 856us/step - loss: 19.0721\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.1692\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.1167\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 830us/step - loss: 19.0584\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.0484\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.1104\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.0849\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.1405\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 994us/step - loss: 19.1470\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 931us/step - loss: 19.0409\n",
      "Epoch 100/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 870us/step - loss: 19.0604\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.0616\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.0884\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.0096\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.0870\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.1941\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.0089\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s 967us/step - loss: 19.0320\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 962us/step - loss: 18.9150\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s 900us/step - loss: 19.1153\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.0221\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s 940us/step - loss: 19.0350\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.1493\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s 893us/step - loss: 19.0263\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s 998us/step - loss: 19.0574\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s 944us/step - loss: 19.0992\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9171\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s 920us/step - loss: 19.0825\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s 939us/step - loss: 19.0936\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s 990us/step - loss: 19.0484\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s 817us/step - loss: 18.9966\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s 870us/step - loss: 19.0432\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.0617\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s 863us/step - loss: 19.0460\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s 903us/step - loss: 18.9848\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9589\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9949\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s 894us/step - loss: 19.0146\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9288\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s 924us/step - loss: 18.9914\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s 856us/step - loss: 18.9874\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9970\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.0614\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s 968us/step - loss: 19.0512\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9776\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9402\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s 854us/step - loss: 18.9963\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9878\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s 915us/step - loss: 19.0568\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s 892us/step - loss: 18.9973\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.0767\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.0661\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s 966us/step - loss: 18.9489\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9205\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.0190\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s 921us/step - loss: 19.0407\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s 951us/step - loss: 19.0850\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9476\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s 971us/step - loss: 18.9125\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s 974us/step - loss: 19.0105\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9088\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s 989us/step - loss: 18.9700\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s 897us/step - loss: 18.9281\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9301\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s 932us/step - loss: 18.9326\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9452\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9950\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s 934us/step - loss: 18.9916\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9114\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9742\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s 915us/step - loss: 18.9518\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.0451\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.8941\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s 855us/step - loss: 18.9294\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s 880us/step - loss: 18.9464\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.0375\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s 908us/step - loss: 18.9349\n",
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s 822us/step - loss: 18.9450\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9895\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9032\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s 837us/step - loss: 18.9987\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9952\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s 984us/step - loss: 18.9327\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s 976us/step - loss: 18.9713\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.8988\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s 870us/step - loss: 18.8764\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s 954us/step - loss: 18.9846\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9899\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9957\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9014\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9198\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s 937us/step - loss: 19.0068\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9003\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9250\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s 937us/step - loss: 18.9076\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.0443\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9674\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s 891us/step - loss: 18.8908\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9308\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9186\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s 783us/step - loss: 18.9356\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9275\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9720\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9817\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.8951\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.8972\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s 942us/step - loss: 18.9620\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9607\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.8811\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9587\n",
      "Epoch 200/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4 [======>.......................] - ETA: 0s - loss: 18.9386\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7835452400>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(8,)),\n",
    "    Dense(2),\n",
    "    tfpl.IndependentNormal(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=lambda y_true, y_pred: -y_pred.log_prob(y_true))\n",
    "model.fit(x_train, y_train, epochs =200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-commonwealth",
   "metadata": {},
   "source": [
    "### Modeling epistemic uncertainty\n",
    "\n",
    "After training this model we obtain point estimates for these weights and biases but in order to capture the epistemic uncertainty (uncertainty in the parameters) we want to learn the posterior distribution over these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-carnival",
   "metadata": {},
   "source": [
    "#### Prior distribution\n",
    "\n",
    "We start by defining the prior distribution over these paramters. This distribution represents our belief of what model parameters are likely before we've seen any data.\n",
    "\n",
    "A standar assumption is that the prior distribution is a spherical gaussian. In other words it is an idependent normal distribution for each weight and bias, all with equal variance.\n",
    "\n",
    "The function defined below will be used to define this prior distribution for a given dense layer. It takes the `kernel_size`, which is the number of parameters in the dense layer weights matrix and the `bias_size`, which is the number of bias parameters. It returns a callable object, in this case a lambda function. It takes an input tensor t and returns an independent normal distribution with mean zero and standard deviation 1.\n",
    "\n",
    "This independent normal is the prior over the dense layer parameters. Notice that the mean is defined with a zero tensor of lenght $n$, where $n$ is the sum of the kernel size and bias size. The input tensor t will be input tensor to the dense layer that we will be defining this prior distribution for. Notice that t is not used in this distribution. In this example we are defining the prior distribution to be the same independent normal distribution regardless of the dense layer input. The prior also don't have any trainable variables , it won't change during the optimization procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "nearby-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(kernel_size, bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return lambda t: tfd.Independent(tfd.Normal(loc=tf.zeros(n, dtype=dtype), scale=1),\n",
    "                                    reinterpreted_batch_ndims=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-stone",
   "metadata": {},
   "source": [
    "#### Posterior distribution\n",
    "\n",
    "It receives the same paramters as the prior function. But this time we are returning a sequential model, which is in fact also a callable object. It receives a tensor as input and returns, once again, a distribution object. The `VariableLayer` is a simple layer type that returns a tensorflow variable when called regardless of the input. So the posterior distribution will also be independent of the tensor input to the dense layer. The distribution returned by the callable sequential model defines a distribution over the dense layer parameters. Notice that we are using the `convert_to_tensor_fn` to convert our output to a tensor by sampling from the output distribution. We need this tensor for our weights in order to run the forward pass of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "funny-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(kernel_size, bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return Sequential([\n",
    "        tfpl.VariableLayer(tfpl.IndependentNormal.params_size(n), dtype=dtype),\n",
    "        tfpl.IndependentNormal(n, convert_to_tensor_fn=tfd.Distribution.sample)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-coach",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-african",
   "metadata": {},
   "source": [
    "The dense layers were replaced by the `DenseVariational` layers. These layers are similar to the regular dense layers, as you can define the number of units, input shape and activation function. But they require also the definition of a prior and posterior functions. The dense variational layer then adds the KL divergence loss in the optimization of the model. The KL divergence can be scaled according to the `kl_weight` keyword argument. Here we are setting it to be $1/N$, where $N$ is the size of the dataset. Normally, this is the setting we should use for this argument. Remeber that our objective is to maximize the ELBO:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "q &= q(w | \\theta) \\\\\n",
    "E_{W ~ q}[\\log p(D|w)]-KL[q ||p] &= \\sum_i E_{W ~ q}[\\log p(D_i|w)]-KL[q ||p]\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $q$ is the posterior distribution over all the weights of our model. So you can think of $w$ being a vector over all model weights, which are now random variables and $\\theta$ is a vector of parameters that parameterize this posterior. So these will be the means and variances of the independent normal distributions. The ELBO is given by two terms, the first is the expected log likelihood of the data, where the expectation is taken over the weights posterior. The second term is the KL divergence between the posterior and the prior, which you can think of of regularizing the posterior. Assuming i.i.d . data, the first term is equal to the sum of of the log likelihoods over all data points, so the index $i$ runs over the whole dataset. An important point to realize is that the first term is data dependent, while the second term is not. The KL divergence only depends on the posterior and prior distributions over the weights.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{N}{B} \\sum_j E_{W ~ q}[\\log p(D_j|w)]-KL[q ||p]\n",
    "\\end{aligned}$$\n",
    "\n",
    "When we train the model we optimize the model using minibatches. That means the we only compute part of the that total sum over all datapoints. So we need to scale up the first term by a factor of $N/B$, where $N$ is the total dataset size and $B$ is the number of elements in the minibatch. With this adjustment, the first term is an unbiased estimate of the full data likelihood. The second term does not need any adjustment because it does not dependend on the datapoints at all. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{1}{B} \\sum_j E_{W ~ q}[\\log p(D_j|w)]-\\frac{1}{N}KL[q ||p]\n",
    "\\end{aligned}$$\n",
    "\n",
    "Finally, we can compute the per sample loss or, in other words, the average expected log likelihood for each data point. This is what tensorflow computes when we pass in a minibatch of data. It will always average the loss for all elements in the minibatch. This is why we define the `kl_weight` to be $1/N$. As you can see by this derivation, it is the correct way to use to get an unbiased estimate of the true ELBO objective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "referenced-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "neutral-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    tfpl.DenseVariational(16, posterior, prior, kl_weight=1/N,\n",
    "                       activation='relu', input_shape=(8,)),\n",
    "    tfpl.DenseVariational(2, posterior, prior, kl_weight=1/N),\n",
    "    tfpl.IndependentNormal(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-numbers",
   "metadata": {},
   "source": [
    "We are now ready to train our model. We are using the negative log likelihood for our loss. This loss is the first term of the ELBO that we've discussed before just with a change of sign. We are estimating the expected log likelihood with a single sample from the posterior on the forward pass as specified in the `convert_to_tensor_fn` in the posterior function definition. The KL divergence is added by each `DenseVariational` layer in the forward pass and so is included in the objective that we are optimizing. In the `DenseVariational` layer we can use the keyword `kl_use_exact` to define how it should be computed. Depending on the distributions used for the prior and posterior, it may be possible to analytically compute the KL divergence. If it is, then you can set the `kl_use_exact` to true. In the case that it is not possible to compute the KL divergence analitically, it will raise an error. With the `kl_use_exact` set to false (which is the default), the KL divergence loss will be estimated using the weights tensor obtained from the posterior distribution converted to a tensor function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sudden-fifth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - 1s 909us/step - loss: 129037.2996\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 205444293596268.2188\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 943us/step - loss: 15060769.0000\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 128803494859201200.0000\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 940us/step - loss: nan\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 981us/step - loss: nan\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 927us/step - loss: nan\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: nan\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 963us/step - loss: nan\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 976us/step - loss: nan\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 963us/step - loss: nan\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 995us/step - loss: nan\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s 914us/step - loss: nan\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s 972us/step - loss: nan\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 989us/step - loss: nan\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 968us/step - loss: nan\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 960us/step - loss: nan\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 987us/step - loss: nan\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 986us/step - loss: nan\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 981us/step - loss: nan\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 936us/step - loss: nan\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 964us/step - loss: nan\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 965us/step - loss: nan\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 945us/step - loss: nan\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 970us/step - loss: nan\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s 901us/step - loss: nan\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 107/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s 938us/step - loss: nan\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s 879us/step - loss: nan\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s 852us/step - loss: nan\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s 969us/step - loss: nan\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s 844us/step - loss: nan\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s 990us/step - loss: nan\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s 904us/step - loss: nan\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s 968us/step - loss: nan\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s 959us/step - loss: nan\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s 931us/step - loss: nan\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s 909us/step - loss: nan\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s 929us/step - loss: nan\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s 939us/step - loss: nan\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s 957us/step - loss: nan\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s 840us/step - loss: nan\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s 857us/step - loss: nan\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s 973us/step - loss: nan\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s 979us/step - loss: nan\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s 856us/step - loss: nan\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s 980us/step - loss: nan\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f783563e220>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=lambda y_true, y_pred: -y_pred.log_prob(y_true))\n",
    "model.fit(x_train, y_train, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-teaching",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
